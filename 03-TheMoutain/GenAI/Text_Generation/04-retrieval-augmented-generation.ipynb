{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df04b7b",
   "metadata": {},
   "source": [
    "# Limitations with Large Language Models\n",
    "\n",
    "Large Language Models are objectively great. They are flexible, surprisingly cunning, and have a considerable amount of knowledge by themselves. They do come short in some cases, especially when it comes to adapting to new contextual information. Let's say you're trying to build an LLM that answers all the questions you may have about BeCode rules. What does ChatGPT know about BeCode rules, was there any of it in its training data? Probably not much.\n",
    "\n",
    "## How could we have a LLM answer BeCode questions?\n",
    "\n",
    "An LLM has a very long context window, close to 1 million words for ChatGPT, that means close to two books from Game of Thrones can be given to it and it would still be able to answer. We could give it all of BeCode rules as a text in the prompt and have it answer questions based on them. But it still comes with many caveats, mostly that giving a lot of content to an LLM is quite costly in resources and money.\n",
    "\n",
    "Wouldn't it be better if we could just give it the parts of the document useful in the answer to help it on the prompt at hand? The LLM doesn't need to be told about the way moodle works in order to explain when the holidays of the bootcamp happen. The document with Becode Rules is given in `data/becode_rules.txt`.\n",
    "\n",
    "Make it so that the LLM can answer the following prompt by giving it the paragraph from becode_rules that will allow it to answer the prompt, insert this in the code snippet underneath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Python SDK\n",
    "from google import genai\n",
    "\n",
    "API_KEY=\"GEMINI API KEY HERE\"\n",
    "\n",
    "if API_KEY==\"GEMINI API KEY HERE\":\n",
    "    raise Exception('Your API key has to be put instead of \"GEMINI API KEY HERE\"')\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)#Telling google what your API key is\n",
    "\n",
    "question = 'I am sick, I sent an email to Antoine and Cindy, what else should I do?'\n",
    "context = '''PASTE INFORMATION IN THE DOCUMENT ABOUT BEING SICK'''\n",
    "prompt = f'Use the following snippet:\\n {context}\\n\\n To answer this question: {question}'\n",
    "print(\"Prompt:\\n\",prompt)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=prompt\n",
    ")\n",
    "print(\"\\n\\nAnswer:\\n\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361b266",
   "metadata": {},
   "source": [
    "## Word2Vec makes a comeback\n",
    "\n",
    "You just had to give the answer to ChatGPT to have it tell you what to do. Not very handy, might as well go search in the document yourself. But what if there was a way to make a program perform that search automatically?\n",
    "\n",
    "In the previous notebooks, you may have read that we used to turn some words into vectors to encode meaning about them, and that words with similar meanings had similar vectors. Well what if you could do this instead with many words? What if you could do it with paragraphs? Wouldn't that be great.\n",
    "\n",
    "Well as it turns out, you can, you can make [embeddings for paragraphs](https://ai.google.dev/gemini-api/docs/embeddings). You can do that with an entire document, and then use the paragraph who's vectors are similar to your prompt to augment it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        contents=\"I am sick, I sent an email to Antoine and Cindy, what else should I do?\")\n",
    "\n",
    "print(result.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###WORK IN PROGRESS### COMING SOON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
